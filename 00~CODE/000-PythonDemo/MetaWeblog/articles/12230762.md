# http

当用户在浏览器地址栏输入了网址，进行跳转，就相当于发送了一个网络请求
这个过程需要有一定的规则，这个规则就是 http（超文本传输协议）

http的请求方式：
- get
  - 明文传参，比较便捷，但相对来说不安全
  - 参数的长度有限制
- post
  - 比较安全
  - 数据整体没有限制
  - 可以上传文件
- put
  - 不完全的
- delete
  - 删除一些信息
- head

发送网络请求的过程：网址 ==> DNS服务器解析 ==> 拿到服务器ip ==> 访问服务器

请求：
  - 发送网络请求，带一定的数据给服务器，不带也可以
  - 发送的东西存放在请求头

请求头：

1. Accept：文本的格式
2. Accept-Encoding：编码格式
3. Connection：长链接，短链接
4. Cookie：验证用的
5. Host：域名
6. Referer：标志从哪个页面跳转过来的
7. User-Agent：浏览器和用户的信息

响应：

  - 接收到的请求存放在响应头里



# 爬虫入门

爬虫：使用代码模拟用户，批量发送网络请求，批量的获取数据

爬虫的价值：

- 数据的价值
- 数据分析
- 流量
- 指数

合法性：灰色产业，政府法律规定爬虫是违法的，也没有法律规定爬虫是合法的

公司概念：公司让你怕数据库之类的（窃取商业机密）责任在公司

爬虫不可以爬取所有东西：只能爬取理论上用户可以访问的东西

爬虫的分类：

- 通用爬虫
  - 使用搜索引擎：百度、谷歌。。。。。
  - 优势：开放性，速度快
  - 劣势：目标不明确
  - 返回内容：基本上百分之90是用户不需要的，不清楚用户的需求在哪里
- 聚焦爬虫
  - 又称主题网络爬虫
  - 优点：目标明确、对用户的需求非常精准，返回的内容很固定
- 增量式爬虫
  - 翻页，第几页到第几页
- 深度爬虫
  - 静态数据：html、css
  - 动态数据：js代码，加密的js



robots协议：是否允许其他爬虫（特指通用爬虫）爬取某些内容

聚焦爬虫不遵守robots协议

爬虫的工作原理：

1. 确认抓取目标的URL
2. 发送请求，获取响应
3. 解析响应的数据
4. 数据持久化
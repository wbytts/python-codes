{'dateCreated': <DateTime '20200116T04:10:00' at 0x1a4f29e7048>, 'description': '**用到的文件**: \n链接：[https://pan.baidu.com/s/1V3gtif8jTFU62VvUBYhZfQ](https://pan.baidu.com/s/1V3gtif8jTFU62VvUBYhZfQ) \n提取码：mtph\n\n# Logistic Regression\n\n## The data\n\n我们将建立一个逻辑回归模型来预测一个学生是否被大学录取。假设你是一个大学系的管理员，你想根据两次考试的结果来决定每个申请人的录取机会。你有以前的申请人的历史数据，你可以用它作为逻辑回归的训练集。对于每一个培训例子，你有两个考试的申请人的分数和录取决定。为了做到这一点，我们将建立一个分类模型，根据考试成绩估计入学概率。\n\n\n```python\n#三大件\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n```\n\n\n```python\nimport os\npath = \'data\' + os.sep + \'LogiReg_data.txt\'\npdData = pd.read_csv(path, header=None, names=[\'Exam 1\', \'Exam 2\', \'Admitted\'])\npdData.head()\n```\n\n\n\n\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border="1" class="dataframe">\n  <thead>\n    <tr style="text-align: right;">\n      <th></th>\n      <th>Exam 1</th>\n      <th>Exam 2</th>\n      <th>Admitted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>34.623660</td>\n      <td>78.024693</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>30.286711</td>\n      <td>43.894998</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>35.847409</td>\n      <td>72.902198</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>60.182599</td>\n      <td>86.308552</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>79.032736</td>\n      <td>75.344376</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\n\n\n\n```python\npdData.shape\n```\n\n\n\n\n    (100, 3)\n\n\n\n\n```python\npositive = pdData[pdData[\'Admitted\'] == 1] # returns the subset of rows such Admitted = 1, i.e. the set of *positive* examples\nnegative = pdData[pdData[\'Admitted\'] == 0] # returns the subset of rows such Admitted = 0, i.e. the set of *negative* examples\n\nfig, ax = plt.subplots(figsize=(10,5))\nax.scatter(positive[\'Exam 1\'], positive[\'Exam 2\'], s=30, c=\'b\', marker=\'o\', label=\'Admitted\')\nax.scatter(negative[\'Exam 1\'], negative[\'Exam 2\'], s=30, c=\'r\', marker=\'x\', label=\'Not Admitted\')\nax.legend()\nax.set_xlabel(\'Exam 1 Score\')\nax.set_ylabel(\'Exam 2 Score\')\n```\n\n\n\n\n    Text(0, 0.5, \'Exam 2 Score\')\n\n\n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040853764-1081934855.png)\n\n\n## The logistic regression\n\n目标：建立分类器（求解出三个参数 $\\theta_0         \\theta_1         \\theta_2 $）\n\n\n设定阈值，根据阈值判断录取结果\n\n### 要完成的模块\n-  `sigmoid` : 映射到概率的函数\n\n-  `model` : 返回预测结果值\n\n-  `cost` : 根据参数计算损失\n\n-  `gradient` : 计算每个参数的梯度方向\n\n-  `descent` : 进行参数更新\n\n-  `accuracy`: 计算精度\n\n###  `sigmoid` 函数\n\n$$\ng(z) = \\frac{1}{1+e^{-z}}   \n$$\n\n\n```python\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n```\n\n\n```python\nnums = np.arange(-10, 10, step=1) #creates a vector containing 20 equally spaced values from -10 to 10\nfig, ax = plt.subplots(figsize=(12,4))\nax.plot(nums, sigmoid(nums), \'r\')\n```\n\n\n\n\n    [<matplotlib.lines.Line2D at 0x19dbf1cd948>]\n\n\n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040853510-14983100.png)\n\n\n### Sigmoid\n* $g:\\mathbb{R} \\to [0,1]$\n* $g(0)=0.5$\n* $g(- \\infty)=0$\n* $g(+ \\infty)=1$\n\n\n```python\ndef model(X, theta):\n    \n    return sigmoid(np.dot(X, theta.T))\n```\n\n$$\n\\begin{array}{ccc}\n\\begin{pmatrix}\\theta_{0} & \\theta_{1} & \\theta_{2}\\end{pmatrix} & \\times & \\begin{pmatrix}1\\\\\nx_{1}\\\\\nx_{2}\n\\end{pmatrix}\\end{array}=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}\n$$\n\n\n```python\n\npdData.insert(0, \'Ones\', 1) # in a try / except structure so as not to return an error if the block si executed several times\n\n\n# set X (training data) and y (target variable)\norig_data = pdData.as_matrix() # convert the Pandas representation of the data to an array useful for further computations\ncols = orig_data.shape[1]\nX = orig_data[:,0:cols-1]\ny = orig_data[:,cols-1:cols]\n\n# convert to numpy arrays and initalize the parameter array theta\n#X = np.matrix(X.values)\n#y = np.matrix(data.iloc[:,3:4].values) #np.array(y.values)\ntheta = np.zeros([1, 3])\n```\n\n    d:\\python\\py376\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n      """\n    \n\n\n```python\nX[:5]\n```\n\n\n\n\n    array([[ 1.        , 34.62365962, 78.02469282],\n           [ 1.        , 30.28671077, 43.89499752],\n           [ 1.        , 35.84740877, 72.90219803],\n           [ 1.        , 60.18259939, 86.3085521 ],\n           [ 1.        , 79.03273605, 75.34437644]])\n\n\n\n\n```python\ny[:5]\n```\n\n\n\n\n    array([[0.],\n           [0.],\n           [0.],\n           [1.],\n           [1.]])\n\n\n\n\n```python\ntheta\n```\n\n\n\n\n    array([[0., 0., 0.]])\n\n\n\n\n```python\nX.shape, y.shape, theta.shape\n```\n\n\n\n\n    ((100, 3), (100, 1), (1, 3))\n\n\n\n### 损失函数\n将对数似然函数去负号\n\n$$\nD(h_\\theta(x), y) = -y\\log(h_\\theta(x)) - (1-y)\\log(1-h_\\theta(x))\n$$\n求平均损失\n$$\nJ(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n} D(h_\\theta(x_i), y_i)\n$$\n\n\n```python\ndef cost(X, y, theta):\n    left = np.multiply(-y, np.log(model(X, theta)))\n    right = np.multiply(1 - y, np.log(1 - model(X, theta)))\n    return np.sum(left - right) / (len(X))\n```\n\n\n```python\ncost(X, y, theta)\n```\n\n\n\n\n    0.6931471805599453\n\n\n\n### 计算梯度\n\n\n$$\n\\frac{\\partial J}{\\partial \\theta_j}=-\\frac{1}{m}\\sum_{i=1}^n (y_i - h_\\theta (x_i))x_{ij}\n$$\n\n\n\n```python\ndef gradient(X, y, theta):\n    grad = np.zeros(theta.shape)\n    error = (model(X, theta)- y).ravel()\n    for j in range(len(theta.ravel())): #for each parmeter\n        term = np.multiply(error, X[:,j])\n        grad[0, j] = np.sum(term) / len(X)\n    \n    return grad\n```\n\n### Gradient descent\n\n比较3中不同梯度下降方法\n\n\n\n```python\nSTOP_ITER = 0\nSTOP_COST = 1\nSTOP_GRAD = 2\n\ndef stopCriterion(type, value, threshold):\n    #设定三种不同的停止策略\n    if type == STOP_ITER:        return value > threshold\n    elif type == STOP_COST:      return abs(value[-1]-value[-2]) < threshold\n    elif type == STOP_GRAD:      return np.linalg.norm(value) < threshold\n```\n\n\n```python\nimport numpy.random\n#洗牌\ndef shuffleData(data):\n    np.random.shuffle(data)\n    cols = data.shape[1]\n    X = data[:, 0:cols-1]\n    y = data[:, cols-1:]\n    return X, y\n```\n\n\n```python\nimport time\n\ndef descent(data, theta, batchSize, stopType, thresh, alpha):\n    #梯度下降求解\n    \n    init_time = time.time()\n    i = 0 # 迭代次数\n    k = 0 # batch\n    X, y = shuffleData(data)\n    grad = np.zeros(theta.shape) # 计算的梯度\n    costs = [cost(X, y, theta)] # 损失值\n\n    \n    while True:\n        grad = gradient(X[k:k+batchSize], y[k:k+batchSize], theta)\n        k += batchSize #取batch数量个数据\n        if k >= n: \n            k = 0 \n            X, y = shuffleData(data) #重新洗牌\n        theta = theta - alpha*grad # 参数更新\n        costs.append(cost(X, y, theta)) # 计算新的损失\n        i += 1 \n\n        if stopType == STOP_ITER:       value = i\n        elif stopType == STOP_COST:     value = costs\n        elif stopType == STOP_GRAD:     value = grad\n        if stopCriterion(stopType, value, thresh): break\n    \n    return theta, i-1, costs, grad, time.time() - init_time\n```\n\n\n```python\ndef runExpe(data, theta, batchSize, stopType, thresh, alpha):\n    #import pdb; pdb.set_trace();\n    theta, iter, costs, grad, dur = descent(data, theta, batchSize, stopType, thresh, alpha)\n    name = "Original" if (data[:,1]>2).sum() > 1 else "Scaled"\n    name += " data - learning rate: {} - ".format(alpha)\n    if batchSize==n: strDescType = "Gradient"\n    elif batchSize==1:  strDescType = "Stochastic"\n    else: strDescType = "Mini-batch ({})".format(batchSize)\n    name += strDescType + " descent - Stop: "\n    if stopType == STOP_ITER: strStop = "{} iterations".format(thresh)\n    elif stopType == STOP_COST: strStop = "costs change < {}".format(thresh)\n    else: strStop = "gradient norm < {}".format(thresh)\n    name += strStop\n    print ("***{}\\nTheta: {} - Iter: {} - Last cost: {:03.2f} - Duration: {:03.2f}s".format(\n        name, theta, iter, costs[-1], dur))\n    fig, ax = plt.subplots(figsize=(12,4))\n    ax.plot(np.arange(len(costs)), costs, \'r\')\n    ax.set_xlabel(\'Iterations\')\n    ax.set_ylabel(\'Cost\')\n    ax.set_title(name.upper() + \' - Error vs. Iteration\')\n    return theta\n```\n\n### 不同的停止策略\n\n#### 设定迭代次数\n\n\n```python\n#选择的梯度下降方法是基于所有样本的\nn=100\nrunExpe(orig_data, theta, n, STOP_ITER, thresh=5000, alpha=0.000001)\n```\n\n    ***Original data - learning rate: 1e-06 - Gradient descent - Stop: 5000 iterations\n    Theta: [[-0.00027127  0.00705232  0.00376711]] - Iter: 5000 - Last cost: 0.63 - Duration: 1.47s\n    \n\n\n\n\n    array([[-0.00027127,  0.00705232,  0.00376711]])\n\n\n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040853242-1452900663.png)\n\n\n#### 根据损失值停止\n\n设定阈值 1E-6, 差不多需要110 000次迭代 \n\n\n```python\nrunExpe(orig_data, theta, n, STOP_COST, thresh=0.000001, alpha=0.001)\n```\n\n    ***Original data - learning rate: 0.001 - Gradient descent - Stop: costs change < 1e-06\n    Theta: [[-5.13364014  0.04771429  0.04072397]] - Iter: 109901 - Last cost: 0.38 - Duration: 32.65s\n    \n\n\n\n\n    array([[-5.13364014,  0.04771429,  0.04072397]])\n\n\n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040852914-96409699.png)\n\n\n#### 根据梯度变化停止\n\n设定阈值 0.05,差不多需要40 000次迭代\n\n\n```python\nrunExpe(orig_data, theta, n, STOP_GRAD, thresh=0.05, alpha=0.001)\n```\n\n    ***Original data - learning rate: 0.001 - Gradient descent - Stop: gradient norm < 0.05\n    Theta: [[-2.37033409  0.02721692  0.01899456]] - Iter: 40045 - Last cost: 0.49 - Duration: 12.20s\n    \n\n\n\n\n    array([[-2.37033409,  0.02721692,  0.01899456]])\n\n\n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040852613-835613345.png)\n\n\n### 对比不同的梯度下降方法\n\n#### Stochastic descent\n\n\n```python\nrunExpe(orig_data, theta, 1, STOP_ITER, thresh=5000, alpha=0.001)\n```\n\n    ***Original data - learning rate: 0.001 - Stochastic descent - Stop: 5000 iterations\n    Theta: [[-0.36656341 -0.01406809 -0.01956622]] - Iter: 5000 - Last cost: 1.80 - Duration: 0.45s\n    \n\n\n\n\n    array([[-0.36656341, -0.01406809, -0.01956622]])\n\n\n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040852338-1454885543.png)\n\n\n有点爆炸。。。很不稳定,再来试试把学习率调小一些\n\n\n```python\nrunExpe(orig_data, theta, 1, STOP_ITER, thresh=15000, alpha=0.000002)\n```\n\n    ***Original data - learning rate: 2e-06 - Stochastic descent - Stop: 15000 iterations\n    Theta: [[-0.00202316  0.00991808  0.00087764]] - Iter: 15000 - Last cost: 0.63 - Duration: 1.38s\n    \n\n\n\n\n    array([[-0.00202316,  0.00991808,  0.00087764]])\n\n\n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040852069-704105941.png)\n\n\n速度快，但稳定性差，需要很小的学习率\n\n#### Mini-batch descent\n\n\n```python\nrunExpe(orig_data, theta, 16, STOP_ITER, thresh=15000, alpha=0.001)\n```\n\n    ***Original data - learning rate: 0.001 - Mini-batch (16) descent - Stop: 15000 iterations\n    Theta: [[-1.03398289  0.04372859  0.02318741]] - Iter: 15000 - Last cost: 1.05 - Duration: 1.83s\n    \n\n\n\n\n    array([[-1.03398289,  0.04372859,  0.02318741]])\n\n\n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040851687-506660984.png)\n\n\n浮动仍然比较大，我们来尝试下对数据进行标准化\n将数据按其属性(按列进行)减去其均值，然后除以其方差。最后得到的结果是，对每个属性/每列来说所有数据都聚集在0附近，方差值为1\n\n\n```python\nfrom sklearn import preprocessing as pp\n\nscaled_data = orig_data.copy()\nscaled_data[:, 1:3] = pp.scale(orig_data[:, 1:3])\n\nrunExpe(scaled_data, theta, n, STOP_ITER, thresh=5000, alpha=0.001)\n```\n\n    ***Scaled data - learning rate: 0.001 - Gradient descent - Stop: 5000 iterations\n    Theta: [[0.3080807  0.86494967 0.77367651]] - Iter: 5000 - Last cost: 0.38 - Duration: 1.56s\n    \n\n\n\n\n    array([[0.3080807 , 0.86494967, 0.77367651]])\n\n\n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040851429-1929574685.png)\n\n\n它好多了！原始数据，只能达到达到0.61，而我们得到了0.38个在这里！\n所以对数据做预处理是非常重要的\n\n\n```python\nrunExpe(scaled_data, theta, n, STOP_GRAD, thresh=0.02, alpha=0.001)\n```\n\n    ***Scaled data - learning rate: 0.001 - Gradient descent - Stop: gradient norm < 0.02\n    Theta: [[1.0707921  2.63030842 2.41079787]] - Iter: 59422 - Last cost: 0.22 - Duration: 19.58s\n    \n\n\n\n\n    array([[1.0707921 , 2.63030842, 2.41079787]])\n\n\n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040851129-1238215480.png)\n\n\n更多的迭代次数会使得损失下降的更多！\n\n\n```python\ntheta = runExpe(scaled_data, theta, 1, STOP_GRAD, thresh=0.002/5, alpha=0.001)\n```\n\n    ***Scaled data - learning rate: 0.001 - Stochastic descent - Stop: gradient norm < 0.0004\n    Theta: [[1.14964649 2.79191694 2.56889202]] - Iter: 72692 - Last cost: 0.22 - Duration: 8.55s\n    \n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040850696-634340393.png)\n\n\n随机梯度下降更快，但是我们需要迭代的次数也需要更多，所以还是用batch的比较合适！！！\n\n\n```python\nrunExpe(scaled_data, theta, 16, STOP_GRAD, thresh=0.002*2, alpha=0.001)\n```\n\n    ***Scaled data - learning rate: 0.001 - Mini-batch (16) descent - Stop: gradient norm < 0.004\n    Theta: [[1.15785505 2.80909166 2.5880511 ]] - Iter: 1700 - Last cost: 0.22 - Duration: 0.36s\n    \n\n\n\n\n    array([[1.15785505, 2.80909166, 2.5880511 ]])\n\n\n\n\n![png](https://img2018.cnblogs.com/blog/1269496/202001/1269496-20200116040850025-2140747980.png)\n\n\n## 精度\n\n\n```python\n#设定阈值\ndef predict(X, theta):\n    return [1 if x >= 0.5 else 0 for x in model(X, theta)]\n```\n\n\n```python\nscaled_X = scaled_data[:, :3]\ny = scaled_data[:, 3]\npredictions = predict(scaled_X, theta)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint (\'accuracy = {0}%\'.format(accuracy))\n```\n\n    accuracy = 89%\n    \n', 'title': '梯度下降求解逻辑回归', 'categories': ['[随笔分类]AI~机器学习~'], 'enclosure': {'length': 0}, 'link': 'https://www.cnblogs.com/wbyixx/p/12199453.html', 'permalink': 'https://www.cnblogs.com/wbyixx/p/12199453.html', 'postid': '12199453', 'source': {}, 'userid': '-2'}
{'dateCreated': <DateTime '20200123T22:00:00' at 0x1a4f29ca308>, 'description': '# 请求网址获取网页代码\n\n```python\nimport urllib.request\nurl = "http://www.baidu.com"\nresponse = urllib.request.urlopen(url)\ndata = response.read()\n# print(data)\n# 将文件获取的内容转换成字符串\nstr_data = data.decode("utf-8")\nprint(str_data)\n# 将结果保存到文件中\nwith open("baidu.html", "w", encoding="utf-8") as f:\n    f.write(str_data)\n```\n\n# get带参数请求\n\n```python\nimport urllib.request\n\ndef get_method_params(wd):\n    url = "http://www.baidu.com/s?wd="\n    # 拼接字符串\n    final_url = url + wd\n    # 发送网络请求\n    response = urllib.request.urlopen(final_url)\n    print(response.read().decode("utf-8"))\n\nget_method_params("美女")\n```\n直接这么写会报错：\n![](https://img2018.cnblogs.com/blog/1446249/202001/1446249-20200123215635233-1484653907.png)\n\n原因是，网址里面包含了汉字，但是ascii码是没有汉字的，需要转义一下：\n```python\nimport urllib.request\nimport urllib.parse\nimport string\n\ndef get_method_params(wd):\n    url = "http://www.baidu.com/s?wd="\n    # 拼接字符串\n    final_url = url + wd\n    # 将包含汉字的网址进行转义\n    encode_new_url = urllib.parse.quote(final_url, safe=string.printable)\n    # 发送网络请求\n    response = urllib.request.urlopen(encode_new_url)\n    print(response.read().decode("utf-8"))\n\nget_method_params("美女")\n```\n\n# 使用字典拼接参数\n\n```python\nimport urllib.request\nimport urllib.parse\nimport string\n\ndef get_params():\n    url = "http://www.baidu.com/s?w"\n\n    params = {\n        "wd": "美女",\n        "key": "zhang",\n        "value": "san"\n    }\n\n    str_params = urllib.parse.urlencode(params)\n    print(str_params)\n\n    final_url = url + str_params\n    # 将带有中文的url转义\n    encode_url = urllib.parse.quote(final_url, safe=string.printable)\n\n    response = urllib.request.urlopen(encode_url)\n    data = response.read().decode("utf-8")\n    print(data)\n\nget_params()\n```\n\n# 设置请求的超时时间\n\nurlopen的参数，timeout：可以设置请求的超时时间\n\n# post请求\n\n`urllib.request.urlopen(url, data="服务器接收的数据")`\n\n# User-Agent\n\n可以伪装请求头的用户信息\n\n常用的请求头整理：https://www.cnblogs.com/wbyixx/p/12231755.html\n\n```python\nimport urllib.request\nimport urllib.parse\nimport string\nimport random\n\n\ndef get_random_user_agent():\n    import random\n    user_agent_list = [......]\n    random_user_agent = random.choice(user_agent_list)\n    return random_user_agent\n\n\nurl = "http://www.baidu.com"\nrequest = urllib.request.Request(url)\n# 添加请求头信息\nrequest.add_header("User-Agent", get_random_user_agent())\n# 请求数据\nresponse = urllib.request.urlopen(request)\n# print(response.read().decode("utf-8"))\n# 获取请求头信息，注意这里的agent小写\nprint(request.get_header("User-agent"))\n```\n\n# handler处理器的使用\n\n```python\nimport urllib.request\n\n\ndef handler_openner():\n    # urlopen为什么可以请求数据\n    # 根据源码可以看出，是由于 openner，而openner又由handler而来\n\n    url = "http://www.baidu.com"\n    # 创建自己的处理器\n    handler = urllib.request.HTTPHandler()\n    # 创建自己的openner\n    openner = urllib.request.build_opener(handler)\n    # 用openner去请求\n    response = openner.open(url)\n    print(response.read().decode("utf-8"))\n\n\nhandler_openner()\n\n```\n\n# 添加ip代理\n\n## ip代理的分类\n\n免费的：时效性差，错误率高\n付费的：贵，也有失效不能用的\n\n性质分类：\n- 透明：对方知道我们的ip\n- 匿名：对方不知道我们的真实ip，但是知道我们使用了代理\n- 高匿：对方不知道我们的真实ip，也不知道我们使用了代理\n\n## 使用代理ip去请求\n创建 ProxyHandler\n```python\nimport urllib.request\n\n\ndef create_proxy_handler():\n    url = "http://www.baidu.com"\n\n    proxy = {\n        # 免费的写法\n        "http": "http://36.27.28.215:9999"\n    }\n\n    # 代理的处理器\n    proxy_handler = urllib.request.ProxyHandler(proxy)\n\n    # 创建自己的openner\n    openner = urllib.request.build_opener(proxy_handler)\n    # 拿着代理ip去发送请求\n    response = openner.open(url)\n\n    print(response.read().decode("utf-8"))\n\n\ncreate_proxy_handler()\n```\n\n付费的代理写法：\n- `{"http": "username:password@ip:port"}`\n- 使用密码管理器\n\n```python\npassword_manager = urllib.request.HTTPPasswordMgrWithDefaultRealm()\npassword_manager.add_password(None, proxy_uri, user, pwd)\nhandler_auth_proxy = urllib.request.ProxyBasicAuthHandler(password_manager)\nopenner_auth_proxy = urllib.request.build_opener(handler_auth_proxy)\nresponse = openner_auth_proxy.open(url)\n```\n\n# Cookie验证请求\n\n## 手动粘贴cookie\n```python\nimport urllib.request\n\nurl = "需要cookie验证才能访问的链接"\n\nheaders = {\n    \'User-Agent\': \'User-Agent,Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\',\n    "Cookie": \'xxx手动粘贴cookie到这里\'\n}\n\nrequest = urllib.request.Request(url, headers=headers)\n\nresponse = urllib.request.urlopen(request)\n\ndata = response.read()\n\nwith open("data.html", "wb") as f:\n    f.write(data)\n```\n\n## 自动获取cookie\n\n1. 使用代码发送登录请求，获取有效的cookie\n2. 自动带着cookie去请求其他页面\n\n```python\nimport urllib.request\nimport urllib.parse\nfrom http import cookiejar\n\nheaders = {\n    \'User-Agent\': \'User-Agent,Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\',\n}\n\nlogin_url = "..."\ntarget_url = "https://i-beta.cnblogs.com/posts"\n\nlogin_form_data = {\n    "username": "xxx",\n    "password": "xxx",\n    # ......其他参数\n}\n\n# 转码\nlogin_form_data = urllib.parse(login_form_data).encode("utf-8")\n\n# 发送请求，拿到response里的cookie\n\ncookie_jar = cookiejar.CookieJar()\n# 定义有添加cookie功能的处理器\ncookie_handler = urllib.request.HTTPCookieProcessor(cookie_jar)\n# 根据处理器生成openner\nopenner = urllib.request.build_opener(cookie_handler)\n\n# 带着参数，发送post请求\nlogin_request = urllib.request.Request(login_url, headers=headers, data=login_form_data)\n# 如果登录成功，cookiejar会自动保存cookie\nopenner.open(login_request)\n\ntarget_request = urllib.request.Request(target_url, headers=headers)\nresponse = openner.open(target_request)\ndata = response.read()\nprint(data)\n```\n\n# 错误处理\n\n## 常见的Error\n\nHTTPError\n\nUrlError\n', 'title': 'Python爬虫：urllib库的基本使用', 'categories': ['[随笔分类]Python~爬虫~', '[随笔分类]Python~爬虫~'], 'enclosure': {'length': 0}, 'link': 'https://www.cnblogs.com/wbyixx/p/12231527.html', 'permalink': 'https://www.cnblogs.com/wbyixx/p/12231527.html', 'postid': '12231527', 'source': {}, 'userid': '-2'}